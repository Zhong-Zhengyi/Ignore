model_args:
  pretrained_model_name_or_path: "unsloth/Llama-3.2-3B-Instruct"   #  "meta-llama/Llama-3.2-3B-Instruct"
  # attn_implementation: 'flash_attention_2'
  # torch_dtype: bfloat16
  attn_implementation: 'eager'
  torch_dtype: bfloat16
  output_attentions: True
tokenizer_args:
  pretrained_model_name_or_path: "unsloth/Llama-3.2-3B-Instruct"   #"meta-llama/Llama-3.2-3B-Instruct"
template_args:
  # apply_chat_template: True
  # system_prompt: You are a helpful assistant.
  # system_prompt_with_special_tokens: "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant.<|eot_id|>"
  # user_start_tag: "<|start_header_id|>user<|end_header_id|>\n\n"
  # user_end_tag: "<|eot_id|>"
  # asst_start_tag: "<|start_header_id|>assistant\n<|end_header_id|>\n\n"
  # asst_end_tag: "<|eot_id|>"
  # date_string: 10 Apr 2025
  apply_chat_template: False
  system_prompt: You are a helpful assistant.
  system_prompt_with_special_tokens: "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
  user_start_tag: "<|im_start|>user\n"
  user_end_tag: "<|im_end|>\n"
  asst_start_tag: "<|im_start|>assistant\n"
  asst_end_tag: "<|im_end|>\n" 